Make sure ollama is running locally
Check if ollama is accible on this endpoint if the llm is not reachable: curl http://localhost:11434/api/generate